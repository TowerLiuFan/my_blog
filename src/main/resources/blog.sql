/*
 Navicat Premium Data Transfer

 Source Server         : localhost
 Source Server Type    : MySQL
 Source Server Version : 80013
 Source Host           : localhost:3306
 Source Schema         : blog

 Target Server Type    : MySQL
 Target Server Version : 80013
 File Encoding         : 65001

 Date: 31/10/2020 07:15:57
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for hibernate_sequence
-- ----------------------------
DROP TABLE IF EXISTS `hibernate_sequence`;
CREATE TABLE `hibernate_sequence` (
  `next_val` bigint(20) DEFAULT NULL
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of hibernate_sequence
-- ----------------------------
BEGIN;
INSERT INTO `hibernate_sequence` VALUES (57);
INSERT INTO `hibernate_sequence` VALUES (57);
INSERT INTO `hibernate_sequence` VALUES (57);
INSERT INTO `hibernate_sequence` VALUES (57);
INSERT INTO `hibernate_sequence` VALUES (57);
COMMIT;

-- ----------------------------
-- Table structure for t_blog
-- ----------------------------
DROP TABLE IF EXISTS `t_blog`;
CREATE TABLE `t_blog` (
  `id` bigint(20) NOT NULL,
  `admiretion` bit(1) NOT NULL,
  `commentabled` bit(1) NOT NULL,
  `content` longtext CHARACTER SET utf8 COLLATE utf8_bin,
  `create_time` datetime DEFAULT NULL,
  `first_picture` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `flag` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `publish` bit(1) NOT NULL,
  `recommend` bit(1) NOT NULL,
  `share_statement` bit(1) NOT NULL,
  `title` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `update_time` datetime DEFAULT NULL,
  `views` int(11) DEFAULT NULL,
  `type_id` bigint(20) DEFAULT NULL,
  `user_id` bigint(20) DEFAULT NULL,
  `description` varchar(1500) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `appreciation` bit(1) NOT NULL,
  `published` bit(1) NOT NULL,
  `css_type` int(1) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `FK292449gwg5yf7ocdlmswv9w4j` (`type_id`),
  KEY `FK8ky5rrsxh01nkhctmo7d48p82` (`user_id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_blog
-- ----------------------------
BEGIN;
INSERT INTO `t_blog` VALUES (31, b'1', b'1', '有些hive安装文档提到了hdfs dfs -mkdir ，也就是说hdfs也是可以用的，但在2.8.0中已经不那么处理了，之所以还可以使用，是为了向下兼容.\r\n\r\n本文简要介绍一下有关的命令，以便对hadoop的命令有一个大概的影响，并在想使用的时候能够知道从哪里可以获得帮助。\r\n\r\n概述\r\n\r\n在$HADOOP_HOME/bin下可以看到hadoop和hdfs的脚本。\r\n\r\nhdfs的相当一部分的功能可以使用hdoop来替代（目前），但hdfs有自己的一些独有的功能。hadoop主要面向更广泛复杂的功能。\r\n\r\n本文介绍hadoop,hdfs和yarn的命令 ，目的是为了给予自己留下一个大概的映像！\r\n\r\n \r\n\r\n第一部分  hadoop命令\r\n\r\n参见http://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/CommandsManual.html \r\n\r\nUsage: hadoop [--config confdir] [--loglevel loglevel] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]\r\n\r\n \r\n\r\nGENERIC_OPTION	Description	中文说明\r\n-archives <comma separated list of archives>	Specify comma separated archives to be unarchived on the compute machines. Applies only to job.	为某个作业提交一串的压缩文件（以逗号分隔),目的是让作业加压，并在计算节点计算\r\n-conf <configuration file>	Specify an application configuration file.	设定配置文件\r\n-D <property>=<value>	Use value for given property.	让hadoop命令使用特性属性值\r\n-files <comma separated list of files>	Specify comma separated files to be copied to the map reduce cluster. Applies only to job.	设定逗号分隔的文件列表，这些文件被复制到mr几圈。只针对job\r\n-fs <file:///> or <hdfs://namenode:port>	Specify default filesystem URL to use. Overrides ‘fs.defaultFS’ property from configurations.	设定hadoop命令需要用到的文件系统，会覆盖fs.defaultFS的配置\r\n-jt <local> or <resourcemanager:port>	Specify a ResourceManager. Applies only to job.	设定一个资源管理器。只针对job\r\n-libjars <comma seperated list of jars>	Specify comma separated jar files to include in the classpath. Applies only to job.	设定一个逗号分隔的jar文件列表，这些jar会被加入classpath。只针对作业\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n一般情况下，以上的通用选项可以不需要用到。\r\n\r\n下面介绍命令command\r\n\r\narchive\r\nchecknative\r\nclasspath\r\ncredential\r\ndistcp\r\nfs\r\njar\r\nkey\r\ntrace\r\nversion\r\nCLASSNAME\r\n\r\n \r\n\r\n1.1 archive\r\n\r\n创建一个hadoop压缩文件，详细的可以参考 http://hadoop.apache.org/docs/r2.8.0/hadoop-archives/HadoopArchives.html\r\n\r\nhadoop的压缩文件不同于普通的压缩文件，是特有格式（不能使用rar,zip,tar之类的解压缩).后缀是har.压缩目录包含元数据和数据。\r\n\r\n压缩的目的，主要是为了减少可用空间，和传输的数据量。\r\n\r\n注：hadoop官方文档没有过多的解释。如此是否意味着har文件仅仅为mapreduce服务？ 如果我们不用mapreduce,那么是否可以不关注这个。\r\n\r\n创建压缩文件\r\n\r\nhadoop archive -archiveName name -p <parent> [-r <replication factor>] <src>* <dest>\r\n\r\n例如\r\n\r\n把目录/foor/bar下的内容压缩为zoo.har并存储在/outputdir下\r\n\r\nhadoop archive -archiveName zoo.har -p /foo/bar -r 3 /outputdir\r\n\r\n把目录/user/haoop/dir1和/user/hadoop/dir2下的文件压缩为foo.har，并存储到/user/zoo中\r\n\r\nhadoop archive -archiveName foo.har -p /user/ hadoop/dir1 hadoop/dir2 /user/zoo\r\n\r\n解压\r\n\r\n把文件foo.har中的dir目录解压到 /user/zoo/newdir下\r\n\r\nhdfs dfs -cp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir \r\n\r\n以并行方式解压\r\n\r\nhadoop distcp har:///user/zoo/foo.har/dir1 hdfs:/user/zoo/newdir\r\n\r\n查看解压文件\r\n\r\nhdfs dfs -ls -R har:///user/zoo/foo.har/\r\n\r\n1.2 checknative\r\n\r\nhadoop checknative [-a] [-h]\r\n\r\n-a 检查所有的库\r\n\r\n-h 显示帮助\r\n\r\n检查hadoop的原生代码，一般人用不到。具体可以参考 http://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/NativeLibraries.html\r\n\r\n \r\n\r\n1.3 classpath\r\n\r\nhadoop classpath [--glob |--jar <path> |-h |--help]\r\n\r\n打印hadoop jar或者库的类路径\r\n\r\n\r\n1.4 credential\r\n\r\nhadoop credential <subcommand> [options]\r\n\r\n管理凭证供应商的凭证、密码和secret(有关秘密信息）。\r\n\r\n查看帮助\r\n\r\nhadoop credential -list\r\n\r\n注：暂时没有涉略，大概是用于有关安全认证的。\r\n\r\n\r\n1.5 distcp\r\n\r\n功能:复制文件或者目录\r\n\r\n详细参考： http://hadoop.apache.org/docs/r2.8.0/hadoop-distcp/DistCp.html\r\n\r\ndistcp就是distributed copy的缩写（望文生义),主要用于集群内/集群之间 复制文件。需要使用到mapreduce。\r\n\r\n原文用了相当的篇幅介绍这个功能，估计这个功能有不少用处，毕竟搬迁巨量文件还是挺复杂的，值得专门写这个工具。\r\n\r\n简单复制1\r\n\r\n[hadoop@bigdata ~]$ hadoop distcp /tmp/input/hadoop /tmp/input/haoop1\r\n17/06/07 15:57:53 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 15:57:53 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n17/06/07 15:57:54 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=100, sslConfigurationFile=\'null\', copyStrategy=\'uniformsize\', preserveStatus=[], preserveRawXattrs=false, atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[/tmp/input/hadoop], targetPath=/tmp/input/haoop1, targetPathExists=false, filtersFile=\'null\'}\r\n17/06/07 15:57:54 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\r\n17/06/07 15:57:56 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 31; dirCnt = 1\r\n17/06/07 15:57:56 INFO tools.SimpleCopyListing: Build file listing completed.\r\n17/06/07 15:57:56 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\r\n17/06/07 15:57:56 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\r\n17/06/07 15:57:57 INFO tools.DistCp: Number of paths in the copy list: 31\r\n17/06/07 15:57:57 INFO tools.DistCp: Number of paths in the copy list: 31\r\n17/06/07 15:57:58 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\r\n17/06/07 15:57:59 INFO mapreduce.JobSubmitter: number of splits:20\r\n17/06/07 15:58:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1496800112089_0001\r\n17/06/07 15:58:01 INFO impl.YarnClientImpl: Submitted application application_1496800112089_0001\r\n17/06/07 15:58:01 INFO mapreduce.Job: The url to track the job: http://bigdata.lzf:8099/proxy/application_1496800112089_0001/\r\n17/06/07 15:58:01 INFO tools.DistCp: DistCp job-id: job_1496800112089_0001\r\n17/06/07 15:58:01 INFO mapreduce.Job: Running job: job_1496800112089_0001\r\n17/06/07 15:58:24 INFO mapreduce.Job: Job job_1496800112089_0001 running in uber mode : false\r\n\r\n--注：后面太多，省略了\r\n\r\n结果查看下（列出部分）\r\n\r\nhadoop fs -ls /tmp/intput/hadoop1\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -ls /tmp/input/haoop1\r\n17/06/07 16:05:58 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 16:05:58 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\nFound 30 items\r\n-rw-r--r--   1 hadoop supergroup       4942 2017-06-07 15:59 /tmp/input/haoop1/capacity-scheduler.xml\r\n-rw-r--r--   1 hadoop supergroup       1335 2017-06-07 15:58 /tmp/input/haoop1/configuration.xsl\r\n-rw-r--r--   1 hadoop supergroup        318 2017-06-07 15:59 /tmp/input/haoop1/container-executor.cfg\r\n-rw-r--r--   1 hadoop supergroup       1443 2017-06-07 15:59 /tmp/input/haoop1/core-site.xml\r\n-rw-r--r--   1 hadoop supergroup       3804 2017-06-07 16:00 /tmp/input/haoop1/hadoop-env.cmd\r\n-rw-r--r--   1 hadoop supergroup       4755 2017-06-07 16:00 /tmp/input/haoop1/hadoop-env.sh\r\n-rw-r--r--   1 hadoop supergroup       2490 2017-06-07 15:58 /tmp/input/haoop1/hadoop-metrics.properties\r\n-rw-r--r--   1 hadoop supergroup       2598 2017-06-07 15:59 /tmp/input/haoop1/hadoop-metrics2.properties\r\n-rw-r--r--   1 hadoop supergroup       9683 2017-06-07 16:00 /tmp/input/haoop1/hadoop-policy.xml\r\n-rw-r--r--   1 hadoop supergroup       1527 2017-06-07 15:58 /tmp/input/haoop1/hdfs-site.xml\r\n-rw-r--r--   1 hadoop supergroup       1449 2017-06-07 15:59 /tmp/input/haoop1/httpfs-env.sh\r\n-rw-r--r--   1 hadoop supergroup       1657 2017-06-07 15:59 /tmp/input/haoop1/httpfs-log4j.properties\r\n\r\n........\r\n\r\n路径可以使用uri，例如  hadoop distcp hdfs://bigdata.lzf:9001/tmp/input/hadoop  hdfs://bigdata.lzf:9001/tmp/input/hadoop1\r\n\r\n源可以是多个 例如  hadoop distcp hdfs://bigdata.lzf:9001/tmp/input/hadoop   hdfs://bigdata.lzf:9001/tmp/input/test   hdfs://bigdata.lzf:9001/tmp/input/hadoop1\r\n\r\n注意：复制的总是提示\r\n\r\n17/06/07 16:09:52 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\r\n17/06/07 16:09:52 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\r\n\r\n这个不用管，通过8099的配置可以看到，使用的是 mapreduce.task.io.sort.mb,mapreduce.task.io.sort.factor\r\n\r\n\r\n1.6 fs\r\n\r\n这个是比较常用的一个命令，和hdfs dfs基本等价，但还是有一些区别。\r\n\r\nhttp://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/FileSystemShell.html\r\n\r\n说明很详细，用法很简单。\r\n\r\n•appendToFile\r\n•cat\r\n•checksum\r\n•chgrp\r\n•chmod\r\n•chown\r\n•copyFromLocal\r\n•copyToLocal\r\n•count\r\n•cp\r\n•createSnapshot\r\n•deleteSnapshot\r\n•df\r\n•du\r\n•dus\r\n•expunge\r\n•find\r\n•get\r\n•getfacl\r\n•getfattr\r\n•getmerge\r\n•help\r\n•ls\r\n•lsr\r\n•mkdir\r\n•moveFromLocal\r\n•moveToLocal\r\n•mv\r\n•put\r\n•renameSnapshot\r\n•rm\r\n•rmdir\r\n•rmr\r\n•setfacl\r\n•setfattr\r\n•setrep\r\n•stat\r\n•tail\r\n•test\r\n•text\r\n•touchz\r\n•truncate\r\n•usage\r\n这些参数很容易阅读理解，和linux的常见文件系统命令基本一致。\r\n\r\n这里介绍几个有意思，且常用的。\r\n\r\n从本地文件系统复制数据到hadoop uri\r\n\r\nhadoop fs -copyFromLocal <localsrc> URI\r\n\r\n这个命令很多情况下等同于put，只不过前者只能在本地文件系统下用。\r\n\r\n例如：\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -copyFromLocal start-hadoop.sh  /log\r\n17/06/07 17:10:21 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:10:21 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n\r\n--通过uri,强制覆盖\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -copyFromLocal -f  start-hadoop.sh  hdfs://bigdata.lzf:9001/log\r\n17/06/07 17:12:03 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:12:03 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n \r\n\r\n复制uri中文件到本地copyToLocal\r\n\r\nhadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>\r\n\r\n命令等同于get,只不过只能复制到本地中而已。\r\n\r\n例如\r\n\r\nhadoop fs -copyToLocal -f  hdfs://bigdata.lzf:9001/log/start-hadoop.sh /home/hadoop/testdir\r\n\r\n计数count\r\n\r\nhadoop fs -count [-q] [-h] [-v] [-x] [-t [<storage type>]] [-u] <paths>\r\n\r\n这个命令还是挺有用的。\r\n\r\nCount the number of directories, files and bytes under the paths that match the specified file pattern. Get the quota and the usage. The output columns with -count are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE, PATHNAME\r\n\r\n计算目录，文件个数和字节数\r\n\r\n例如：\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -count /tmp/input/hadoop\r\n17/06/07 17:41:04 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:41:04 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n           1           30              83564 /tmp/input/hadoop\r\n\r\n通过这个命令，了解下存储的文件情况。\r\n\r\n复制cp\r\n\r\nUsage: hadoop fs -cp [-f] [-p | -p[topax]] URI [URI ...] <dest>\r\n\r\n目标必须是目录，源可以多个。\r\n\r\n和distcp有点类似，不过这个只能在同个hadoop集群内？ 且distcp需要使用mapreduce\r\n\r\n例如：\r\n\r\nhadoop fs -cp /tmp/input/hadoop1/hadoop/*.* /tmp/input/hadoop\r\n\r\n创建快照\r\n\r\nhttp://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-hdfs/HdfsSnapshots.html\r\n\r\n主要功能是备份\r\n\r\n删除快照\r\n\r\n略\r\n\r\n显示可用空间df\r\n\r\nhadoop fs -df [-h] URI [URI ...]\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -df -h /\r\n17/06/07 17:51:31 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:51:31 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\nFilesystem                 Size   Used  Available  Use%\r\nhdfs://bigdata.lzf:9001  46.5 G  1.5 M     35.8 G    0%\r\n\r\n计算目录字节大小du\r\n\r\nhadoop fs -du [-s] [-h] [-x] URI [URI ...]\r\n\r\n部分功能可以用count替代\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -du -h /\r\n17/06/07 17:52:56 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:52:56 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n0      /input\r\n63     /log\r\n0      /test\r\n1.0 M  /tmp\r\n9      /warehouse\r\n[hadoop@bigdata ~]$ hadoop fs -du -h -s /\r\n17/06/07 17:53:19 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/07 17:53:19 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n1.0 M  /\r\n[hadoop@bigdata ~]$\r\n\r\n参数基本同Linux的\r\n\r\n清空回收站数据expunge\r\n\r\nhadoop fs -expunge\r\n\r\n永久删除过期的文件，并创建新的检查点。 检查点比fs.trash.interval老的数据，会再下次的这个操作的时候清空。\r\n\r\n查找find\r\n\r\nhadoop fs -find <path> ... <expression> ...\r\n\r\n查找根据文件名称查找，而不是文件内容。\r\n\r\n[hadoop@bigdata ~]$ hadoop fs -find / -name hadoop  -print\r\n17/06/08 11:59:04 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/08 11:59:04 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n/tmp/hadoop-yarn/staging/hadoop\r\n/tmp/hadoop-yarn/staging/history/done_intermediate/hadoop\r\n/tmp/hive/hadoop\r\n/tmp/input/hadoop\r\n/tmp/input/hadoop1/hadoop\r\n\r\n或者 使用iname(不考虑大小写)\r\n\r\n hadoop fs -find / -iname hadoop  -print  \r\n\r\n[hadoop@bigdata ~]$ hadoop fs -find / -name hadooP  -print\r\n17/06/08 12:00:59 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/08 12:00:59 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n[hadoop@bigdata ~]$ hadoop fs -find / -iname hadooP  -print\r\n17/06/08 12:01:06 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\r\n17/06/08 12:01:06 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\r\n/tmp/hadoop-yarn/staging/hadoop\r\n/tmp/hadoop-yarn/staging/history/done_intermediate/hadoop\r\n/tmp/hive/hadoop\r\n/tmp/input/hadoop\r\n/tmp/input/hadoop1/hadoop\r\n\r\n \r\n\r\n下载文件到本地get\r\n\r\n类似于copyToLocal.但有crc校验\r\n\r\nhadoop fs -get [-ignorecrc] [-crc] [-p] [-f] <src> <localdst>\r\n\r\n例如：\r\n\r\nhadoop fs -get /tmp/input/hadoop/*.xml /home/hadoop/testdir/\r\n\r\n查看文件或者目录属性 getfattr\r\n\r\nhadoop fs -getfattr [-R] -n name | -d [-e en] <path>\r\n\r\n-n name和 -d是互斥的，-d表示获取所有属性。-R表示循环获取； -e en 表示对获取的内容编码，en的可以取值是 “text”, “hex”, and “base64”.\r\n\r\n例如\r\n\r\nhadoop fs -getfattr -d /file\r\n\r\nhadoop fs -getfattr -R -n user.myAttr /dir\r\n\r\n从实际例子看，暂时不知道有什么特别用处。\r\n\r\n \r\n\r\n合并文件getmerge\r\n\r\nhadoop fs -getmerge -nl  /src  /opt/output.txt\r\n\r\nhadoop fs -getmerge -nl  /src/file1.txt /src/file2.txt  /output.txt\r\n\r\n例如：\r\n\r\nhadoop fs -getmerge -nl /tmp/input/hadoop/hadoop-env.sh /tmp/input/hadoop/slaves /home/hadoop/testdir/merget-test.txt\r\n\r\n注：目标是本地文件，不是uri文件\r\n\r\n罗列文件列表ls\r\n\r\nhadoop fs -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] <args>\r\n\r\nmkdir hadoop fs -mkdir [-p] <paths>   --创建目录\r\n\r\nmoveFromLocal  hadoop fs -moveFromLocal <localsrc> <dst>  --从本地上传，类似Put\r\n\r\n集群内移动目录mv   \r\n\r\nhadoop fs -mv URI [URI ...] <dest>    \r\n\r\n源可以是多个。\r\n\r\n例如  hadoop fs -mv /tmp/input/hadoop1/hadoop/slaves /tmp/input/hadoop1/\r\n\r\n上传文件put\r\n\r\nhadoop fs -put  [-f] [-p] [-l] [-d] [ - | <localsrc1>  .. ]. <dst>\r\n\r\n类似于copyFromLocal\r\n\r\n删除文件rm      hadoop fs -rm [-f] [-r |-R] [-skipTrash] [-safely] URI [URI ...]\r\n\r\n删除目录rmdir  hadoop fs -rmdir [--ignore-fail-on-non-empty] URI [URI ...]\r\n\r\n显示文件部分内容tail  hadoop fs -tail [-f] URI \r\n\r\n其余略\r\n\r\n\r\n1.7 jar\r\n\r\n使用hadoop来运行一个jar\r\n\r\nhadoop jar <jar> [mainClass] args...\r\n\r\n但hadoop建议使用yarn jar 来替代hadoop jar\r\n\r\nyarn jar的命令参考   http://hadoop.apache.org/docs/r2.8.0/hadoop-yarn/hadoop-yarn-site/YarnCommands.html#jar\r\n\r\n\r\n1.8 key\r\n\r\n管理密匙供应商的密匙\r\n\r\n具体略\r\n1.9 trace\r\n\r\n查看和修改跟踪设置，具体参考  http://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/Tracing.html\r\n1.10 version\r\n\r\n查看版本信息\r\n\r\nhadoop version\r\n\r\n\r\n1.11 CLASSNAME\r\n\r\n利用hadoop运行某个类\r\n\r\n语法：hadoop CLASSNAME\r\n\r\n以下内容来自 http://www.thebigdata.cn/Hadoop/1924.html\r\n\r\n使用hadoop CLASSNAM之前，你需要设置HADOOP_CLASSPATH.\r\n\r\nexport HADOOP_CLASSPATH=/home/hadoop/jardir/*.jar:/home/hadoop/workspace/hdfstest/bin/\r\n\r\n其中/home/hadoop/jardir/包含了我所有的hadoop的jar包。\r\n\r\n/home/hadoop/workspace/hdfstest/bin/就是我的开发class的所在目录。\r\n\r\n我使用eclipse写java开发，由于eclipse有自动编译的功能，写好之后，就可以直接在命令行运行hadoop CLASSNAME的命令：hadoop FileSystemDoubleCat hdfs://Hadoop:8020/xstartup\r\n\r\n你同样可以将你的工程打成runable jar包（将所有的jar包打包）。 然后运行hadoop jar jar包名 类型 参数1 。 每一次都要打成jar包，这对于测试来说极不方便的。。。\r\n\r\n这个主要就是为了方便开发人员测试的。', '2019-01-23 08:27:09', 'https://picsum.photos/800/400/?gravity=east', '原创', b'1', b'1', b'1', '介绍hadoop中的hadoop和hdfs命令', '2020-10-22 07:49:31', 8, 33, 1, '有些hive安装文档提到了hdfs dfs -mkdir ，也就是说hdfs也是可以用的，但在2.8.0中已经不那么处理了，之所以还可以使用，是为了向下兼容.\r\n\r\n本文简要介绍一下有关的命令，以便对Hadoop的命令有一个大概的影响，并在想使用的时候能够知道从哪里可以获得帮助。', b'1', b'1', 1);
INSERT INTO `t_blog` VALUES (32, b'0', b'1', '修改一下\r\nFS Shell\r\n调用文件系统(FS)Shell命令应使用 bin/hadoop fs <args>的形式。 所有的的FS shell命令使用URI路径作为参数。URI格式是scheme://authority/path。对HDFS文件系统，scheme是hdfs，对本地文件系统，scheme是file。其中scheme和authority参数都是可选的，如果未加指定，就会使用配置中指定的默认scheme。一个HDFS文件或目录比如/parent/child可以表示成hdfs://namenode:namenodeport/parent/child，或者更简单的/parent/child（假设你配置文件中的默认值是namenode:namenodeport）。大多数FS Shell命令的行为和对应的Unix Shell命令类似，不同之处会在下面介绍各命令使用详情时指出。出错信息会输出到stderr，其他信息输出到stdout。\r\n\r\ncat\r\n使用方法：hadoop fs -cat URI [URI …]\r\n\r\n将路径指定文件的内容输出到stdout。\r\n\r\n示例：\r\n\r\nhadoop fs -cat hdfs://host1:port1/file1 hdfs://host2:port2/file2\r\nhadoop fs -cat file:///file3 /user/hadoop/file4\r\n返回值：\r\n成功返回0，失败返回-1。\r\n\r\nchgrp\r\n使用方法：hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the Permissions User Guide. -->\r\n\r\n改变文件所属的组。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。\r\n\r\nchmod\r\n使用方法：hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI …]\r\n\r\n改变文件的权限。使用-R将使改变在目录结构下递归进行。命令的使用者必须是文件的所有者或者超级用户。更多的信息请参见HDFS权限用户指南。\r\n\r\nchown\r\n使用方法：hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]\r\n\r\n改变文件的拥有者。使用-R将使改变在目录结构下递归进行。命令的使用者必须是超级用户。更多的信息请参见HDFS权限用户指南。\r\n\r\ncopyFromLocal\r\n使用方法：hadoop fs -copyFromLocal <localsrc> URI\r\n\r\n除了限定源路径是一个本地文件外，和put命令相似。\r\n\r\ncopyToLocal\r\n使用方法：hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>\r\n\r\n除了限定目标路径是一个本地文件外，和get命令类似。\r\n\r\ncp\r\n使用方法：hadoop fs -cp URI [URI …] <dest>\r\n\r\n将文件从源路径复制到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。 \r\n示例：\r\n\r\nhadoop fs -cp /user/hadoop/file1 /user/hadoop/file2\r\nhadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\ndu\r\n使用方法：hadoop fs -du URI [URI …]\r\n\r\n显示目录中所有文件的大小，或者当只指定一个文件时，显示此文件的大小。\r\n示例：\r\nhadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://host:port/user/hadoop/dir1 \r\n返回值：\r\n成功返回0，失败返回-1。 \r\ndus\r\n使用方法：hadoop fs -dus <args>\r\n\r\n显示文件的大小。\r\n\r\nexpunge\r\n使用方法：hadoop fs -expunge\r\n\r\n清空回收站。请参考HDFS设计文档以获取更多关于回收站特性的信息。\r\n\r\nget\r\n使用方法：hadoop fs -get [-ignorecrc] [-crc] <src> <localdst> \r\n复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件。使用-crc选项复制文件以及CRC信息。\r\n\r\n示例：\r\n\r\nhadoop fs -get /user/hadoop/file localfile\r\nhadoop fs -get hdfs://host:port/user/hadoop/file localfile\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\ngetmerge\r\n使用方法：hadoop fs -getmerge <src> <localdst> [addnl]\r\n\r\n接受一个源目录和一个目标文件作为输入，并且将源目录中所有的文件连接成本地目标文件。addnl是可选的，用于指定在每个文件结尾添加一个换行符。\r\n\r\nls\r\n使用方法：hadoop fs -ls <args>\r\n\r\n如果是文件，则按照如下格式返回文件信息：\r\n文件名 <副本数> 文件大小 修改日期 修改时间 权限 用户ID 组ID \r\n如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：\r\n目录名 <dir> 修改日期 修改时间 权限 用户ID 组ID \r\n示例：\r\nhadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile \r\n返回值：\r\n成功返回0，失败返回-1。 \r\nlsr\r\n使用方法：hadoop fs -lsr <args> \r\nls命令的递归版本。类似于Unix中的ls -R。\r\n\r\nmkdir\r\n使用方法：hadoop fs -mkdir <paths> \r\n接受路径指定的uri作为参数，创建这些目录。其行为类似于Unix的mkdir -p，它会创建路径中的各级父目录。\r\n\r\n示例：\r\n\r\nhadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2\r\nhadoop fs -mkdir hdfs://host1:port1/user/hadoop/dir hdfs://host2:port2/user/hadoop/dir\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nmovefromLocal\r\n使用方法：dfs -moveFromLocal <src> <dst>\r\n\r\n输出一个”not implemented“信息。\r\n\r\nmv\r\n使用方法：hadoop fs -mv URI [URI …] <dest>\r\n\r\n将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。 \r\n示例：\r\n\r\nhadoop fs -mv /user/hadoop/file1 /user/hadoop/file2\r\nhadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nput\r\n使用方法：hadoop fs -put <localsrc> ... <dst>\r\n\r\n从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。\r\nhadoop fs -put localfile /user/hadoop/hadoopfile\r\nhadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir\r\nhadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile\r\nhadoop fs -put - hdfs://host:port/hadoop/hadoopfile \r\n从标准输入中读取输入。\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nrm\r\n使用方法：hadoop fs -rm URI [URI …]\r\n\r\n删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。\r\n示例：\r\n\r\nhadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nrmr\r\n使用方法：hadoop fs -rmr URI [URI …]\r\n\r\ndelete的递归版本。\r\n示例：\r\n\r\nhadoop fs -rmr /user/hadoop/dir\r\nhadoop fs -rmr hdfs://host:port/user/hadoop/dir\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nsetrep\r\n使用方法：hadoop fs -setrep [-R] <path>\r\n\r\n改变一个文件的副本系数。-R选项用于递归改变目录下所有文件的副本系数。\r\n\r\n示例：\r\n\r\nhadoop fs -setrep -w 3 -R /user/hadoop/dir1\r\n返回值：\r\n\r\n成功返回0，失败返回-1。\r\n\r\nstat\r\n使用方法：hadoop fs -stat URI [URI …]\r\n\r\n返回指定路径的统计信息。\r\n\r\n示例：\r\n\r\nhadoop fs -stat path\r\n返回值：\r\n成功返回0，失败返回-1。\r\n\r\ntail\r\n使用方法：hadoop fs -tail [-f] URI\r\n\r\n将文件尾部1K字节的内容输出到stdout。支持-f选项，行为和Unix中一致。\r\n\r\n示例：\r\n\r\nhadoop fs -tail pathname\r\n返回值：\r\n成功返回0，失败返回-1。\r\n\r\ntest\r\n使用方法：hadoop fs -test -[ezd] URI\r\n\r\n选项：\r\n-e 检查文件是否存在。如果存在则返回0。\r\n-z 检查文件是否是0字节。如果是则返回0。 \r\n-d 如果路径是个目录，则返回1，否则返回0。\r\n示例：\r\n\r\nhadoop fs -test -e filename\r\ntext\r\n使用方法：hadoop fs -text <src> \r\n将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。\r\n\r\ntouchz\r\n使用方法：hadoop fs -touchz URI [URI …] \r\n创建一个0字节的空文件。\r\n\r\n示例：\r\n\r\nhadoop -touchz pathname\r\n返回值：\r\n成功返回0，失败返回-1。', NULL, 'https://picsum.photos/800/450/?random', '转载', b'1', b'0', b'1', 'Hadoop Shell命令', '2020-10-28 08:00:27', 17, 5, 1, 'FS Shell\r\n调用文件系统(FS)Shell命令应使用 bin/Hadoop fs <args>的形式。 所有的的FS shell命令使用URI路径作为参数', b'0', b'0', 2);
INSERT INTO `t_blog` VALUES (34, b'0', b'1', '# 基于Beego开发的个人博客\r\n\r\n>  作者： 莫棄\r\n\r\n\r\n\r\n**个人博客功能：**\r\n\r\n![](https://ws2.sinaimg.cn/large/006tKfTcgy1fk7m27hbn4j31ds0ycdnp.jpg)\r\n\r\n\r\n**内容模块：**\r\n\r\n*  需求分析与功能规划\r\n*  页面设计与开发\r\n*  技术框架搭建\r\n*  后端管理功能实现\r\n*  前端管理功能实现\r\n\r\n\r\n## 1、需求与功能\r\n\r\n### 1.1 用户故事\r\n\r\n用户故事是敏捷框架中的一种开发方法。可以帮助开发者转换视角，以用户的角度更好的把握需求，从而实现具有商业价值的功能。\r\n\r\n>  用户故事最好是用户团队编写\r\n\r\n**用户故事模板**：\r\n\r\n-  As a (role of user), I want (some feature) so that (some business value).\r\n-  作为一个(某个角色) 使用者，我可以做(某个功能) 事情，如此可以有(某个商业价值) 的好处\r\n\r\n**关键点**：角色、功能、商业价值\r\n\r\n**举例**：\r\n\r\n-  作为一个招聘网站**注册用户**，我想**查看最近3天发布的招聘信息**，以便于**了解最新的招聘信息**。\r\n-  作为公司，可以张贴新工作。\r\n\r\n\r\n\r\n个人博客系统的用户故事：\r\n\r\n角色：**普通访客**，**管理员（我）**\r\n\r\n*  访客，可以分页查看所有的博客\r\n*  访客，可以快速查看博客数最多的6个分类\r\n*  访客，可以查看所有的分类\r\n*  访客，可以查看某个分类下的博客列表\r\n*  访客，可以快速查看标记博客最多的10个标签\r\n*  访客，可以查看所有的标签\r\n*  访客，可以查看某个标签下的博客列表\r\n*  访客，可以根据年度时间线查看博客列表\r\n*  访客，可以快速查看最新的推荐博客\r\n*  访客，可以用关键字全局搜索博客\r\n*  访客，可以查看单个博客内容\r\n*  访客，可以对博客内容进行评论\r\n*  访客，可以赞赏博客内容\r\n*  访客，可以微信扫码阅读博客内容\r\n*  访客，可以在首页扫描公众号二维码关注我\r\n*  我，可以用户名和密码登录后台管理\r\n*  我，可以管理博客\r\n   *  我，可以发布新博客\r\n   *  我，可以对博客进行分类\r\n   *  我，可以对博客打标签\r\n   *  我，可以修改博客\r\n   *  我，可以删除博客\r\n   *  我，可以根据标题，分类，标签查询博客\r\n*  我，可以管理博客分类\r\n   *  我，可以新增一个分类\r\n   *  我，可以修改一个分类\r\n   *  我，可以删除一个分类\r\n   *  我，可以根据分类名称查询分类\r\n*  我，可以管理标签\r\n   *  我，可以新增一个标签\r\n   *  我，可以修改一个标签\r\n   *  我，可以删除一个标签\r\n   *  我，可以根据名称查询标签\r\n\r\n### 1.2 功能规划\r\n\r\n![](https://ws2.sinaimg.cn/large/006tKfTcgy1fk7m27hbn4j31ds0ycdnp.jpg)\r\n\r\n## 2、页面设计与开发\r\n\r\n### 2.1 设计\r\n\r\n**页面规划：**\r\n\r\n前端展示：首页、详情页、分类、标签、归档、关于我\r\n\r\n后台管理：模板页\r\n\r\n### 2.2 页面开发\r\n\r\n\r\n\r\n[Semantic UI官网](https://semantic-ui.com/)\r\n\r\n[Semantic UI中文官网](http://www.semantic-ui.cn/)\r\n\r\n[WebStorm下载与破解](https://imcoding.me/blogs/5)\r\n\r\n[背景图片资源](https://www.toptal.com/designers/subtlepatterns/)\r\n\r\n### 2.3 插件集成\r\n\r\n\r\n\r\n[编辑器 Markdown](https://pandao.github.io/editor.md/)\r\n\r\n[内容排版 typo.css](https://github.com/sofish/typo.css)\r\n\r\n[动画 animate.css](https://daneden.github.io/animate.css/)\r\n\r\n[代码高亮 prism](https://github.com/PrismJS/prism)\r\n\r\n[目录生成 Tocbot](https://tscanlin.github.io/tocbot/)\r\n\r\n[滚动侦测 waypoints](http://imakewebthings.com/waypoints/)\r\n\r\n[平滑滚动 jquery.scrollTo](https://github.com/flesler/jquery.scrollTo)\r\n\r\n[二维码生成 qrcode.js](https://davidshimjs.github.io/qrcodejs/)\r\n\r\n## 3、框架搭建\r\n\r\n>  [IDEA下载 https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)\r\n\r\n### 3.1 构建与配置\r\n\r\n**1、引入Spring Boot模块：**\r\n\r\n*  web\r\n*  Thymeleaf\r\n*  JPA\r\n*  MySQL\r\n*  Aspects\r\n*  DevTools\r\n\r\n**2、application.yml配置**\r\n\r\n*  使用 thymeleaf 3\r\n\r\n   pom.xml:\r\n\r\n```xml\r\n<thymeleaf.version>3.0.2.RELEASE</thymeleaf.version>\r\n<thymeleaf-layout-dialect.version>2.1.1</thymeleaf-layout-dialect.version>\r\n```\r\n\r\n\r\n  	application.yml:\r\n\r\n```yaml\r\nspring:\r\n  thymeleaf:\r\n    mode: HTML\r\n```\r\n\r\n*  数据库连接配置\r\n\r\n```yaml\r\nspring:\r\n  datasource:\r\n    driver-class-name: com.mysql.jdbc.Driver\r\n    url: jdbc:mysql://localhost:3306/blog?useUnicode=true&characterEncoding=utf-8\r\n    username: root\r\n    password: root\r\n  jpa:\r\n    hibernate:\r\n      ddl-auto: update\r\n    show-sql: true\r\n```\r\n\r\n*  日志配置\r\n\r\n   application.yml:\r\n\r\n```yaml\r\nlogging:\r\n  level:\r\n    root: info\r\n    com.imcoding: debug\r\n  file: log/imcoding.log\r\n```\r\n\r\n​	logback-spring.xml：\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\r\n<configuration>\r\n    <!--包含Spring boot对logback日志的默认配置-->\r\n    <include resource=\"org/springframework/boot/logging/logback/defaults.xml\" />\r\n    <property name=\"LOG_FILE\" value=\"${LOG_FILE:-${LOG_PATH:-${LOG_TEMP:-${java.io.tmpdir:-/tmp}}}/spring.log}\"/>\r\n    <include resource=\"org/springframework/boot/logging/logback/console-appender.xml\" />\r\n\r\n    <!--重写了Spring Boot框架 org/springframework/boot/logging/logback/file-appender.xml 配置-->\r\n    <appender name=\"TIME_FILE\"\r\n              class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\r\n        <encoder>\r\n            <pattern>${FILE_LOG_PATTERN}</pattern>\r\n        </encoder>\r\n        <file>${LOG_FILE}</file>\r\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\r\n            <fileNamePattern>${LOG_FILE}.%d{yyyy-MM-dd}.%i</fileNamePattern>\r\n            <!--保留历史日志一个月的时间-->\r\n            <maxHistory>30</maxHistory>\r\n            <!--\r\n            Spring Boot默认情况下，日志文件10M时，会切分日志文件,这样设置日志文件会在100M时切分日志\r\n            -->\r\n            <timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\">\r\n                <maxFileSize>10MB</maxFileSize>\r\n            </timeBasedFileNamingAndTriggeringPolicy>\r\n\r\n        </rollingPolicy>\r\n    </appender>\r\n\r\n    <root level=\"INFO\">\r\n        <appender-ref ref=\"CONSOLE\" />\r\n        <appender-ref ref=\"TIME_FILE\" />\r\n    </root>\r\n\r\n</configuration>\r\n<!--\r\n    1、继承Spring boot logback设置（可以在appliaction.yml或者application.properties设置logging.*属性）\r\n    2、重写了默认配置，设置日志文件大小在100MB时，按日期切分日志，切分后目录：\r\n\r\n        my.2017-08-01.0   80MB\r\n        my.2017-08-01.1   10MB\r\n        my.2017-08-02.0   56MB\r\n        my.2017-08-03.0   53MB\r\n        ......\r\n-->\r\n```\r\n\r\n*  生产环境与开发环境配置\r\n   *  application-dev.yml\r\n   *  application-pro.yml\r\n\r\n### 3.2 异常处理\r\n\r\n**1、定义错误页面**\r\n\r\n*  404\r\n*  500\r\n*  error\r\n\r\n**2、全局处理异常**\r\n\r\n统一处理异常：\r\n\r\n```java\r\n@ControllerAdvice\r\npublic class ControllerExceptionHandler {\r\n\r\n    private final Logger logger = LoggerFactory.getLogger(ControllerExceptionHandler.class);\r\n    /**\r\n     * 异常处理\r\n     * @param request\r\n     * @param e\r\n     * @return\r\n     */\r\n    @ExceptionHandler({Exception.class})\r\n    public ModelAndView handleException(HttpServletRequest request, Exception e) throws Exception {\r\n\r\n        logger.error(\"Request URL : {} , Exception : {}\", request.getRequestURL(), e);\r\n\r\n        if (AnnotationUtils.findAnnotation(e.getClass(), ResponseStatus.class) != null) {\r\n            throw e;\r\n        }\r\n        ModelAndView mav = new ModelAndView();\r\n        mav.addObject(\"url\", request.getRequestURL());\r\n        mav.addObject(\"exception\", e);\r\n        mav.setViewName(\"error/error\");\r\n\r\n        return mav;\r\n    }\r\n}\r\n```\r\n\r\n\r\n\r\n错误页面异常信息显示处理：\r\n\r\n```html\r\n<div>\r\n    <div th:utext=\"\'&lt;!--\'\" th:remove=\"tag\"></div>\r\n    <div th:utext=\"\'Failed Request URL : \' + ${url}\" th:remove=\"tag\"></div>\r\n    <div th:utext=\"\'Exception message : \' + ${exception.message}\" th:remove=\"tag\"></div>\r\n    <ul th:remove=\"tag\">\r\n        <li th:each=\"st : ${exception.stackTrace}\" th:remove=\"tag\"><span th:utext=\"${st}\" th:remove=\"tag\"></span></li>\r\n    </ul>\r\n    <div th:utext=\"\'--&gt;\'\" th:remove=\"tag\"></div>\r\n</div>\r\n```\r\n\r\n\r\n\r\n**3、资源找不到异常**\r\n\r\n```java\r\n@ResponseStatus(HttpStatus.NOT_FOUND)\r\npublic class NotFoundExcepiton extends RuntimeException {\r\n\r\n    public NotFoundExcepiton() {\r\n    }\r\n\r\n    public NotFoundExcepiton(String message) {\r\n        super(message);\r\n    }\r\n\r\n    public NotFoundExcepiton(String message, Throwable cause) {\r\n        super(message, cause);\r\n    }\r\n}\r\n```\r\n\r\n\r\n\r\n### 3.3 日志处理\r\n\r\n**1、记录日志内容**\r\n\r\n*  请求 url\r\n*  访问者 ip\r\n*  调用方法 classMethod\r\n*  参数 args\r\n*  返回内容\r\n\r\n**2、记录日志类：**\r\n\r\n```java\r\n@Aspect\r\n@Component\r\npublic class LogAspect {\r\n\r\n    private final Logger logger = LoggerFactory.getLogger(this.getClass());\r\n\r\n    /**\r\n     * 定义切面\r\n     */\r\n    @Pointcut(\"execution(* com.imcoding.web.*.*(..))\")\r\n    public void log() {\r\n    }\r\n\r\n    @Before(\"log()\")\r\n    public void doBefore(JoinPoint joinPoint) {\r\n        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();\r\n        HttpServletRequest request = attributes.getRequest();\r\n        String classMethod = joinPoint.getSignature().getDeclaringTypeName() + \".\" + joinPoint.getSignature().getName();\r\n        ReqeustLog reqeustLog = new ReqeustLog(\r\n                request.getRequestURL().toString(),\r\n                request.getRemoteAddr(),\r\n                classMethod,\r\n                joinPoint.getArgs()\r\n        );\r\n        logger.info(\"Rquest  ----- {}\",reqeustLog);\r\n    }\r\n\r\n    @After(\"log()\")\r\n    public void doAfter() {\r\n        //logger.info(\"---------- doAfter 2 ----------\");\r\n    }\r\n\r\n    @AfterReturning(returning = \"result\",pointcut = \"log()\")\r\n    public void doAtfertRturning(Object result) {\r\n        logger.info(\"Return ------ {}\",result );\r\n    }\r\n\r\n\r\n    private class ReqeustLog {\r\n        private String url;\r\n        private String ip;\r\n        private String classMethod;\r\n        private Object[] args;\r\n\r\n        public ReqeustLog(String url, String ip, String classMethod, Object[] args) {\r\n            this.url = url;\r\n            this.ip = ip;\r\n            this.classMethod = classMethod;\r\n            this.args = args;\r\n        }\r\n\r\n        @Override\r\n        public String toString() {\r\n            return \"ReqeustLog{\" +\r\n                    \"url=\'\" + url + \'\\\'\' +\r\n                    \", ip=\'\" + ip + \'\\\'\' +\r\n                    \", classMethod=\'\" + classMethod + \'\\\'\' +\r\n                    \", args=\" + Arrays.toString(args) +\r\n                    \'}\';\r\n        }\r\n    }\r\n\r\n}\r\n```\r\n\r\n\r\n\r\n### 3.4 页面处理\r\n\r\n\r\n\r\n**1、静态页面导入project**\r\n\r\n**2、thymeleaf布局**\r\n\r\n*  定义fragment\r\n*  使用fragment布局\r\n\r\n**3、错误页面美化**\r\n\r\n4、设计与规范\r\n\r\n### 4.1 实体设计\r\n\r\n**实体类：**\r\n\r\n*  博客 Blog\r\n*  博客分类 Type\r\n*  博客标签 Tag\r\n*  博客评论 Comment\r\n*  用户 User\r\n\r\n\r\n\r\n**实体关系：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/87092095.jpg)\r\n\r\n**评论类自关联关系：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/41296045.jpg)\r\n\r\n**Blog类：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/38390041.jpg)\r\n\r\n**Type类：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/22984471.jpg)\r\n\r\n**Tag类：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/70860077.jpg)\r\n\r\n**Comment类：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/77104424.jpg)\r\n\r\n\r\n\r\n**User类：**\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/10367795.jpg)\r\n\r\n\r\n\r\n### 4.2 应用分层\r\n\r\n![](http://on91wk3hn.bkt.clouddn.com/17-10-14/64528736.jpg)\r\n\r\n### 4.3 命名约定\r\n\r\n**Service/DAO层命名约定：**\r\n\r\n*  获取单个对象的方法用get做前缀。\r\n*  获取多个对象的方法用list做前缀。\r\n*  获取统计值的方法用count做前缀。\r\n*  插入的方法用save(推荐)或insert做前缀。\r\n*  删除的方法用remove(推荐)或delete做前缀。\r\n*  修改的方法用update做前缀。\r\n\r\n\r\n\r\n\r\n![](https://ws2.sinaimg.cn/large/006tKfTcgy1fk7m27hbn4j31ds0ycdnp.jpg)\r\n\r\n## 5、后台管理功能实现\r\n\r\n### 5.1 登录\r\n\r\n\r\n\r\n**1、构建登录页面和后台管理首页**\r\n\r\n**2、UserService和UserRepository**\r\n\r\n**3、LoginController实现登录**\r\n\r\n**4、MD5加密**\r\n\r\n**5、登录拦截器**\r\n\r\n### 5.2 分类管理\r\n\r\n\r\n\r\n**1、分类管理页面**\r\n\r\n**2、分类列表分页**\r\n\r\n````javascript\r\n{\r\n  \"content\":[\r\n    {\"id\":123,\"title\":\"blog122\",\"content\":\"this is blog content\"},\r\n    {\"id\":122,\"title\":\"blog121\",\"content\":\"this is blog content\"},\r\n    {\"id\":121,\"title\":\"blog120\",\"content\":\"this is blog content\"},\r\n    {\"id\":120,\"title\":\"blog119\",\"content\":\"this is blog content\"},\r\n    {\"id\":119,\"title\":\"blog118\",\"content\":\"this is blog content\"},\r\n    {\"id\":118,\"title\":\"blog117\",\"content\":\"this is blog content\"},\r\n    {\"id\":117,\"title\":\"blog116\",\"content\":\"this is blog content\"},\r\n    {\"id\":116,\"title\":\"blog115\",\"content\":\"this is blog content\"},\r\n    {\"id\":115,\"title\":\"blog114\",\"content\":\"this is blog content\"},\r\n    {\"id\":114,\"title\":\"blog113\",\"content\":\"this is blog content\"},\r\n    {\"id\":113,\"title\":\"blog112\",\"content\":\"this is blog content\"},\r\n    {\"id\":112,\"title\":\"blog111\",\"content\":\"this is blog content\"},\r\n    {\"id\":111,\"title\":\"blog110\",\"content\":\"this is blog content\"},\r\n    {\"id\":110,\"title\":\"blog109\",\"content\":\"this is blog content\"},\r\n    {\"id\":109,\"title\":\"blog108\",\"content\":\"this is blog content\"}],\r\n  \"last\":false,\r\n  \"totalPages\":9,\r\n  \"totalElements\":123,\r\n  \"size\":15,\r\n  \"number\":0,\r\n  \"first\":true,\r\n  \"sort\":[{\r\n    \"direction\":\"DESC\",\r\n    \"property\":\"id\",\r\n    \"ignoreCase\":false,\r\n    \"nullHandling\":\"NATIVE\",\r\n    \"ascending\":false\r\n  }],\r\n  \"numberOfElements\":15\r\n}\r\n````\r\n\r\n\r\n\r\n**3、分类新增、修改、删除**\r\n\r\n### 5.3 标签管理\r\n\r\n### 5.4 博客管理\r\n\r\n\r\n\r\n**1、博客分页查询**\r\n\r\n**2、博客新增**\r\n\r\n**3、博客修改**\r\n\r\n**4、博客删除**\r\n\r\n## 6、前端展示功能实现\r\n\r\n### 6.1 首页展示\r\n\r\n\r\n\r\n**1、博客列表**\r\n\r\n**2、top分类**\r\n\r\n**3、top标签**\r\n\r\n**4、最新博客推荐**\r\n\r\n**5、博客详情**\r\n\r\n**1、Markdown 转换 HTML**\r\n\r\n*  [commonmark-java  https://github.com/atlassian/commonmark-java](https://github.com/atlassian/commonmark-java)\r\n*  pom.xml引用commonmark和扩展插件\r\n\r\n```xml\r\n<dependency>\r\n   <groupId>com.atlassian.commonmark</groupId>\r\n   <artifactId>commonmark</artifactId>\r\n   <version>0.10.0</version>\r\n</dependency>\r\n<dependency>\r\n   <groupId>com.atlassian.commonmark</groupId>\r\n   <artifactId>commonmark-ext-heading-anchor</artifactId>\r\n   <version>0.10.0</version>\r\n</dependency>\r\n<dependency>\r\n   <groupId>com.atlassian.commonmark</groupId>\r\n   <artifactId>commonmark-ext-gfm-tables</artifactId>\r\n   <version>0.10.0</version>\r\n</dependency>\r\n```\r\n\r\n\r\n\r\n**2、评论功能**\r\n\r\n\r\n\r\n*  评论信息提交与回复功能\r\n*  评论信息列表展示功能\r\n*  管理员回复评论功能\r\n\r\n\r\n### 6.2 分类页\r\n\r\n### 6.3 标签页\r\n\r\n### 6.4 归档页\r\n\r\n### 6.5 关于我', '2019-01-24 08:53:47', 'https://picsum.photos/800/400/?random', '翻译', b'1', b'1', b'1', '测试MarkDown', '2020-10-22 09:54:36', 80, 33, 1, '这是一个博客描述修改1111', b'1', b'1', 3);
COMMIT;

-- ----------------------------
-- Table structure for t_blog_tags
-- ----------------------------
DROP TABLE IF EXISTS `t_blog_tags`;
CREATE TABLE `t_blog_tags` (
  `blogs_id` bigint(20) NOT NULL,
  `tags_id` bigint(20) NOT NULL,
  KEY `FK5feau0gb4lq47fdb03uboswm8` (`tags_id`),
  KEY `FKh4pacwjwofrugxa9hpwaxg6mr` (`blogs_id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_blog_tags
-- ----------------------------
BEGIN;
INSERT INTO `t_blog_tags` VALUES (31, 49);
INSERT INTO `t_blog_tags` VALUES (32, 49);
INSERT INTO `t_blog_tags` VALUES (34, 49);
INSERT INTO `t_blog_tags` VALUES (32, 20);
INSERT INTO `t_blog_tags` VALUES (31, 20);
INSERT INTO `t_blog_tags` VALUES (34, 20);
INSERT INTO `t_blog_tags` VALUES (34, 1);
INSERT INTO `t_blog_tags` VALUES (31, 1);
INSERT INTO `t_blog_tags` VALUES (32, 1);
COMMIT;

-- ----------------------------
-- Table structure for t_comment
-- ----------------------------
DROP TABLE IF EXISTS `t_comment`;
CREATE TABLE `t_comment` (
  `id` bigint(20) NOT NULL,
  `avatar` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `content` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `create_time` datetime DEFAULT NULL,
  `email` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `nickname` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `blog_id` bigint(20) DEFAULT NULL,
  `comment_parent_id` bigint(20) DEFAULT NULL,
  `admin_comment` bit(1) NOT NULL,
  `parent_comment_id` bigint(20) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `FKke3uogd04j4jx316m1p51e05u` (`blog_id`),
  KEY `FKm43a84yi4xxp2mm7hpv58fd1i` (`comment_parent_id`),
  KEY `FK4jj284r3pb7japogvo6h72q95` (`parent_comment_id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_comment
-- ----------------------------
BEGIN;
INSERT INTO `t_comment` VALUES (39, '/images/avatar.png', '测试评论', '2019-01-26 23:44:16', '2740640064@qq.com', '小凡', 34, NULL, b'0', NULL);
INSERT INTO `t_comment` VALUES (40, '/images/avatar.png', '这是回复@小凡的一个评论', '2019-01-26 23:44:40', '2740640064@qq.com', '小叶子', 34, 39, b'0', NULL);
INSERT INTO `t_comment` VALUES (47, 'https://picsum.photos/100/100?image=1005', '你好啊，小叶子', '2019-01-27 01:19:01', '2740640064@qq.com', '小黄', 34, 40, b'1', NULL);
INSERT INTO `t_comment` VALUES (42, '/images/avatar.png', '大家好！！！', '2019-01-27 00:46:13', '2740640064@qq.com', '小蓝', 34, NULL, b'0', NULL);
INSERT INTO `t_comment` VALUES (43, 'https://picsum.photos/100/100?image=1005', '博主评论信息', '2019-01-27 01:09:08', '2740640064@qq.com', '莫棄', 34, NULL, b'1', NULL);
INSERT INTO `t_comment` VALUES (44, 'https://picsum.photos/100/100?image=1005', '你好，我是博主！！', '2019-01-27 01:10:50', '2740640064@qq.com', '莫棄', 34, 41, b'1', NULL);
INSERT INTO `t_comment` VALUES (45, 'https://picsum.photos/100/100?image=1005', '你好，小叶子！！', '2019-01-27 01:11:32', '2740640064@qq.com', '莫棄', 34, 40, b'1', NULL);
INSERT INTO `t_comment` VALUES (46, 'https://picsum.photos/100/100?image=1005', '你好，小凡！！', '2019-01-27 01:15:24', '2740640064@qq.com', '莫棄', 34, 41, b'1', NULL);
INSERT INTO `t_comment` VALUES (48, '/images/avatar.png', '大家好啊', '2019-01-27 10:00:31', '2740640064@qq.com', '小黄', 29, NULL, b'0', NULL);
INSERT INTO `t_comment` VALUES (50, '/images/avatar.png', '你好', '2019-02-07 01:26:10', '2740640064@qq.com', '小蓝', 34, NULL, b'0', NULL);
INSERT INTO `t_comment` VALUES (51, 'https://picsum.photos/100/100?image=1005', '你好', '2019-02-07 01:26:56', '2740640064@qq.com', '莫棄', 34, 50, b'1', NULL);
COMMIT;

-- ----------------------------
-- Table structure for t_tag
-- ----------------------------
DROP TABLE IF EXISTS `t_tag`;
CREATE TABLE `t_tag` (
  `id` bigint(20) NOT NULL,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_tag
-- ----------------------------
BEGIN;
INSERT INTO `t_tag` VALUES (1, 'hadoop');
INSERT INTO `t_tag` VALUES (20, '自媒体');
INSERT INTO `t_tag` VALUES (49, 'linux');
COMMIT;

-- ----------------------------
-- Table structure for t_type
-- ----------------------------
DROP TABLE IF EXISTS `t_type`;
CREATE TABLE `t_type` (
  `id` bigint(20) NOT NULL,
  `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_type
-- ----------------------------
BEGIN;
INSERT INTO `t_type` VALUES (5, '学习日志');
INSERT INTO `t_type` VALUES (33, '开发日志');
COMMIT;

-- ----------------------------
-- Table structure for t_user
-- ----------------------------
DROP TABLE IF EXISTS `t_user`;
CREATE TABLE `t_user` (
  `id` bigint(20) NOT NULL,
  `avatar` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `create_time` datetime DEFAULT NULL,
  `email` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `nickname` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `password` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `type` int(11) DEFAULT NULL,
  `update_time` datetime DEFAULT NULL,
  `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of t_user
-- ----------------------------
BEGIN;
INSERT INTO `t_user` VALUES (1, 'https://picsum.photos/100/100?image=1005', '2019-01-20 23:07:02', '2740640064@qq.com', '莫棄', '96e79218965eb72c92a549dd5a330112', 1, '2019-01-20 23:07:06', 'liufan');
COMMIT;

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user` (
  `ID` int(11) NOT NULL AUTO_INCREMENT,
  `Name` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL,
  `Age` int(11) DEFAULT NULL,
  PRIMARY KEY (`ID`)
) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin;

-- ----------------------------
-- Records of user
-- ----------------------------
BEGIN;
INSERT INTO `user` VALUES (1, '刘凡', 24);
INSERT INTO `user` VALUES (2, '王叶', 23);
INSERT INTO `user` VALUES (3, '吉尔伽美什', 100);
INSERT INTO `user` VALUES (4, '石原莉奈', 22);
COMMIT;

SET FOREIGN_KEY_CHECKS = 1;
